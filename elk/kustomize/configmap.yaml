apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    <source>
      @type  forward
      @id    input1
      @label @mainstream
      port  24224
    </source>
    <filter **>
      @type stdout
    </filter>
    <label @mainstream>
      <match docker.**>
        @type file
        @id   output_docker1
        path         /fluentd/log/docker.*.log
        symlink_path /fluentd/log/docker.log
        append       true
        time_slice_format %Y%m%d
        time_slice_wait   1m
        time_format       %Y%m%dT%H%M%S%z
      </match>
      <match **>
        @type file
        @id   output1
        path         /fluentd/log/data.*.log
        symlink_path /fluentd/log/data.log
        append       true
        time_slice_format %Y%m%d
        time_slice_wait   10m
        time_format       %Y%m%dT%H%M%S%z
      </match>
    </label>

  fluentd.conf: |
    # AUTOMATICALLY GENERATED
    # DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb
    
    #@include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
    @include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
    @include kubernetes.conf
    @include conf.d/*.conf
    
    <match **>
      @type kafka_buffered
      @id out_kafka
    
      brokers "#{ENV['FLUENT_KAFKA_BROKERS']}"
    
      default_topic "#{ENV['FLUENT_KAFKA_DEFAULT_TOPIC'] || nil}"
      default_partition_key "#{ENV['FLUENT_KAFKA_DEFAULT_PARTITION_KEY'] || nil}"
      default_message_key "#{ENV['FLUENT_KAFKA_DEFAULT_MESSAGE_KEY'] || nil}"
      output_data_type "#{ENV['FLUENT_KAFKA_OUTPUT_DATA_TYPE'] || 'json'}"
      output_include_tag "#{ENV['FLUENT_KAFKA_OUTPUT_INCLUDE_TAG'] || false}"
      output_include_time "#{ENV['FLUENT_KAFKA_OUTPUT_INCLUDE_TIME'] || false}"
      exclude_topic_key "#{ENV['FLUENT_KAFKA_EXCLUDE_TOPIC_KEY'] || false}"
      exclude_partition_key "#{ENV['FLUENT_KAFKA_EXCLUDE_PARTITION_KEY'] || false}"
      get_kafka_client_log "#{ENV['FLUENT_KAFKA_GET_KAFKA_CLIENT_LOG'] || false}"
    
      # ruby-kafka producer options
      max_send_retries "#{ENV['FLUENT_KAFKA_MAX_SEND_RETRIES'] || 1}"
      required_acks "#{ENV['FLUENT_KAFKA_REQUIRED_ACKS'] || -1}"
      ack_timeout "#{ENV['FLUENT_KAFKA_ACK_TIMEOUT'] || nil}"
      compression_codec "#{ENV['FLUENT_KAFKA_COMPRESSION_CODEC'] || nil}"
      max_send_limit_bytes "#{ENV['FLUENT_KAFKA_MAX_SEND_LIMIT_BYTES'] || nil}"
      discard_kafka_delivery_failed "#{ENV['FLUENT_KAFKA_DISCARD_KAFKA_DELIVERY_FAILED'] || false}" 

  kubernetes.conf: |
    # AUTOMATICALLY GENERATED
    # DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/kubernetes.conf.erb
    
    <label @FLUENT_LOG>
      <match fluent.**>
        @type null
      </match>
    </label>
    
    <source>
        @type tail
        @id in_tail_container_logs
        path /var/log/containers/*.log
        pos_file /var/log/fluentd-containers.log.pos
        tag "#{ENV['FLUENT_CONTAINER_TAIL_TAG'] || 'kubernetes.*'}"
        exclude_path "#{ENV['FLUENT_CONTAINER_TAIL_EXCLUDE_PATH'] || use_default}"
        read_from_head true
    #  <parse>
    #    @type "#{ENV['FLUENT_CONTAINER_TAIL_PARSER_TYPE'] || 'json'}"
    #    time_format %Y-%m-%dT%H:%M:%S.%NZ
    #  </parse>
        <parse>
            @type multi_format
            <pattern>
                format json
                time_key time
                time_type string
                time_format "%Y-%m-%dT%H:%M:%S.%NZ"
                keep_time_key false
            </pattern>
            <pattern>
                format regexp
                expression /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/
                time_format '%Y-%m-%dT%H:%M:%S.%N%:z'
                keep_time_key false
            </pattern>
        </parse>
    </source>
    
    <filter kubernetes.var.log.containers.**.log>
      @type kubernetes_metadata
    </filter>
    
    <filter kubernetes.**>
      @type parser
      key_name log
      <parse>
        @type json
        json_parser json
      </parse>
      replace_invalid_sequence true
      reserve_data true # this preserves unparsable log lines
      emit_invalid_record_to_error false # In case of unparsable log lines keep the error log clean
      reserve_time # the time was already parsed in the source, we don't want to overwrite it with current time.
    </filter>
 
  prometheus.conf: |
     # AUTOMATICALLY GENERATED
     # DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/prometheus.conf.erb
     
     # Prometheus metric exposed on 0.0.0.0:24231/metrics
     <source>
       @type prometheus
       bind "#{ENV['FLUENTD_PROMETHEUS_BIND'] || '0.0.0.0'}"
       port "#{ENV['FLUENTD_PROMETHEUS_PORT'] || '24231'}"
       metrics_path "#{ENV['FLUENTD_PROMETHEUS_PATH'] || '/metrics'}"
     </source>
     
     <source>
       @type prometheus_output_monitor
     </source>

  systemd.conf: |
     # AUTOMATICALLY GENERATED
     # DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/systemd.conf.erb
     
     # Logs from systemd-journal for interesting services.
     <source>
       @type systemd
       @id in_systemd_kubelet
       matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
       <storage>
         @type local
         persistent true
         path /var/log/fluentd-journald-kubelet-cursor.json
       </storage>
       read_from_head true
       tag kubelet
     </source>
     
     # Logs from docker-systemd
     <source>
       @type systemd
       @id in_systemd_docker
       matches [{ "_SYSTEMD_UNIT": "docker.service" }]
       <storage>
         @type local
         persistent true
         path /var/log/fluentd-journald-docker-cursor.json
       </storage>
       read_from_head true
       tag docker.systemd
     </source>
     
     # Logs from systemd-journal for interesting services.
     <source>
       @type systemd
       @id in_systemd_bootkube
       matches [{ "_SYSTEMD_UNIT": "bootkube.service" }]
       <storage>
         @type local
         persistent true
         path /var/log/fluentd-journald-bootkube-cursor.json
       </storage>
       read_from_head true
       tag bootkube
     </source>
     
